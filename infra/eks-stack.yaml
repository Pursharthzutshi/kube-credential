# AWSTemplateFormatVersion: '2010-09-09'
# Description: Minimal EKS cluster with managed node group and ECR repos for zupple project

# Parameters:
#   ClusterName:
#     Type: String
#     Default: zupple-eks
#   KubernetesVersion:
#     Type: String
#     Default: '1.29'
#   NodeInstanceType:
#     Type: String
#     Default: t3.medium
#   NodeDesiredSize:
#     Type: Number
#     Default: 2
#   NodeMinSize:
#     Type: Number
#     Default: 1
#   NodeMaxSize:
#     Type: Number
#     Default: 3
#   VpcCidr:
#     Type: String
#     Default: 10.0.0.0/16

# Resources:
#   VPC:
#     Type: AWS::EC2::VPC
#     Properties:
#       CidrBlock: !Ref VpcCidr
#       EnableDnsSupport: true
#       EnableDnsHostnames: true
#       Tags:
#         - Key: Name
#           Value: !Sub '${ClusterName}-vpc'

#   InternetGateway:
#     Type: AWS::EC2::InternetGateway

#   VPCGatewayAttachment:
#     Type: AWS::EC2::VPCGatewayAttachment
#     Properties:
#       VpcId: !Ref VPC
#       InternetGatewayId: !Ref InternetGateway

#   PublicSubnet1:
#     Type: AWS::EC2::Subnet
#     Properties:
#       VpcId: !Ref VPC
#       CidrBlock: 10.0.0.0/20
#       AvailabilityZone: !Select [ 0, !GetAZs '' ]
#       MapPublicIpOnLaunch: true
#       Tags:
#         - Key: Name
#           Value: !Sub '${ClusterName}-public-1'
#         - Key: kubernetes.io/role/elb
#           Value: '1'

#   PublicSubnet2:
#     Type: AWS::EC2::Subnet
#     Properties:
#       VpcId: !Ref VPC
#       CidrBlock: 10.0.16.0/20
#       AvailabilityZone: !Select [ 1, !GetAZs '' ]
#       MapPublicIpOnLaunch: true
#       Tags:
#         - Key: Name
#           Value: !Sub '${ClusterName}-public-2'
#         - Key: kubernetes.io/role/elb
#           Value: '1'

#   PrivateSubnet1:
#     Type: AWS::EC2::Subnet
#     Properties:
#       VpcId: !Ref VPC
#       CidrBlock: 10.0.32.0/20
#       AvailabilityZone: !Select [ 0, !GetAZs '' ]
#       MapPublicIpOnLaunch: false
#       Tags:
#         - Key: Name
#           Value: !Sub '${ClusterName}-private-1'
#         - Key: kubernetes.io/role/internal-elb
#           Value: '1'

#   PrivateSubnet2:
#     Type: AWS::EC2::Subnet
#     Properties:
#       VpcId: !Ref VPC
#       CidrBlock: 10.0.48.0/20
#       AvailabilityZone: !Select [ 1, !GetAZs '' ]
#       MapPublicIpOnLaunch: false
#       Tags:
#         - Key: Name
#           Value: !Sub '${ClusterName}-private-2'
#         - Key: kubernetes.io/role/internal-elb
#           Value: '1'

#   PublicRouteTable:
#     Type: AWS::EC2::RouteTable
#     Properties:
#       VpcId: !Ref VPC

#   PublicRoute:
#     Type: AWS::EC2::Route
#     Properties:
#       RouteTableId: !Ref PublicRouteTable
#       DestinationCidrBlock: 0.0.0.0/0
#       GatewayId: !Ref InternetGateway

#   PublicSubnet1RouteTableAssociation:
#     Type: AWS::EC2::SubnetRouteTableAssociation
#     Properties:
#       RouteTableId: !Ref PublicRouteTable
#       SubnetId: !Ref PublicSubnet1

#   PublicSubnet2RouteTableAssociation:
#     Type: AWS::EC2::SubnetRouteTableAssociation
#     Properties:
#       RouteTableId: !Ref PublicRouteTable
#       SubnetId: !Ref PublicSubnet2

#   EKSClusterRole:
#     Type: AWS::IAM::Role
#     Properties:
#       AssumeRolePolicyDocument:
#         Version: '2012-10-17'
#         Statement:
#           - Effect: Allow
#             Principal:
#               Service: eks.amazonaws.com
#             Action: sts:AssumeRole
#       ManagedPolicyArns:
#         - arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
#         - arn:aws:iam::aws:policy/AmazonEKSServicePolicy

#   EKSCluster:
#     Type: AWS::EKS::Cluster
#     Properties:
#       Name: !Ref ClusterName
#       Version: !Ref KubernetesVersion
#       RoleArn: !GetAtt EKSClusterRole.Arn
#       ResourcesVpcConfig:
#         SubnetIds:
#           - !Ref PublicSubnet1
#           - !Ref PublicSubnet2
#           - !Ref PrivateSubnet1
#           - !Ref PrivateSubnet2
#         EndpointPublicAccess: true
#         EndpointPrivateAccess: false

#   NodeInstanceRole:
#     Type: AWS::IAM::Role
#     Properties:
#       AssumeRolePolicyDocument:
#         Version: '2012-10-17'
#         Statement:
#           - Effect: Allow
#             Principal:
#               Service: ec2.amazonaws.com
#             Action: sts:AssumeRole
#       ManagedPolicyArns:
#         - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
#         - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
#         - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
#         - arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy

#   NodeGroup:
#     Type: AWS::EKS::Nodegroup
#     Properties:
#       ClusterName: !Ref ClusterName
#       NodeRole: !GetAtt NodeInstanceRole.Arn
#       ScalingConfig:
#         DesiredSize: !Ref NodeDesiredSize
#         MinSize: !Ref NodeMinSize
#         MaxSize: !Ref NodeMaxSize
#       Subnets:
#         - !Ref PublicSubnet1
#         - !Ref PublicSubnet2
#       InstanceTypes:
#         - !Ref NodeInstanceType

#   EbsCsiAddon:
#     Type: AWS::EKS::Addon
#     Properties:
#       ClusterName: !Ref ClusterName
#       AddonName: aws-ebs-csi-driver
#       ResolveConflicts: NONE

#   FrontendEcr:
#     Type: AWS::ECR::Repository
#     Properties:
#       RepositoryName: zupple-frontend
#       ImageScanningConfiguration:
#         ScanOnPush: true

#   IssuanceEcr:
#     Type: AWS::ECR::Repository
#     Properties:
#       RepositoryName: zupple-issuance
#       ImageScanningConfiguration:
#         ScanOnPush: true

#   VerificationEcr:
#     Type: AWS::ECR::Repository
#     Properties:
#       RepositoryName: zupple-verification
#       ImageScanningConfiguration:
#         ScanOnPush: true

# Outputs:
#   ClusterNameOut:
#     Value: !Ref ClusterName
#     Export:
#       Name: !Sub '${ClusterName}-name'
#   ClusterArn:
#     Value: !Ref EKSCluster
#   ClusterOidcIssuer:
#     Value: !GetAtt EKSCluster.OpenIdConnectIssuerUrl
#   PublicSubnetIds:
#     Value: !Join [ ',', [ !Ref PublicSubnet1, !Ref PublicSubnet2 ] ]
#   PrivateSubnetIds:
#     Value: !Join [ ',', [ !Ref PrivateSubnet1, !Ref PrivateSubnet2 ] ]
#   FrontendEcrUri:
#     Value: !Sub '${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/zupple-frontend'
#   IssuanceEcrUri:
#     Value: !Sub '${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/zupple-issuance'
#   VerificationEcrUri:
#     Value: !Sub '${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/zupple-verification'



# name: Deploy to EKS

# on:
#   workflow_dispatch:
#     inputs:
#       aws-region:
#         description: 'AWS region'
#         default: 'ap-south-1'
#         required: true
#       cluster-name:
#         description: 'EKS Cluster name'
#         default: 'zupple-eks'
#         required: true
#       node-instance-type:
#         description: 'EC2 instance type for nodes'
#         default: 't3.medium'
#         required: true

# jobs:
#   deploy:
#     runs-on: ubuntu-latest
#     steps:
#       - name: Checkout
#         uses: actions/checkout@v4

#       - name: Configure AWS credentials
#         uses: aws-actions/configure-aws-credentials@v2
#         with:
#           aws-region: ${{ github.event.inputs['aws-region'] }}
#           role-to-assume: ${{ secrets.AWS_ROLE_ARN }}    # or use access key inputs/secrets
#           aws-web-identity-token-file: ${{ secrets.AWS_WEB_IDENTITY_TOKEN_FILE }} # example

#       - name: Create / update CloudFormation stack
#         run: |
#           aws cloudformation deploy \
#             --stack-name "${{ github.event.inputs['cluster-name'] }}-stack" \
#             --region "${{ github.event.inputs['aws-region'] }}" \
#             --template-file eks-stack.yaml \
#             --parameter-overrides ClusterName="${{ github.event.inputs['cluster-name'] }}" \
#                                   NodeInstanceType="${{ github.event.inputs['node-instance-type'] }}" \
#                                   NodeDesiredSize=2 \
#             --capabilities CAPABILITY_NAMED_IAM


name: Deploy to EKS

on:
  workflow_dispatch:
    inputs:
      aws_region:
        description: AWS region
        required: true
        default: us-east-1
      cluster_name:
        description: EKS cluster name
        required: true
        default: zupple-eks
      node_instance_type:
        description: Node instance type
        required: true
        default: t3.medium
      deploy_frontend:
        description: Deploy frontend component
        required: true
        default: 'false'
        type: choice
        options: ['true','false']
      deploy_issuance:
        description: Deploy issuance backend
        required: true
        default: 'true'
        type: choice
        options: ['true','false']
      deploy_verification:
        description: Deploy verification backend
        required: true
        default: 'true'
        type: choice
        options: ['true','false']
      issuance_url:
        description: Issuance backend URL (for frontend build)
        required: false
        default: ''
      verification_url:
        description: Verification backend URL (for frontend build)
        required: false
        default: ''

jobs:
  deploy:
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    env:
      AWS_REGION: ${{ github.event.inputs.aws_region }}
      CLUSTER_NAME: ${{ github.event.inputs.cluster_name }}
      DEPLOY_FRONTEND: ${{ github.event.inputs.deploy_frontend }}
      DEPLOY_ISSUANCE: ${{ github.event.inputs.deploy_issuance }}
      DEPLOY_VERIFICATION: ${{ github.event.inputs.deploy_verification }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install deps (kubectl/jq)
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq
          curl -sSLo kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.29.0/2024-08-12/bin/linux/amd64/kubectl
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/
          curl -sSLo aws-iam-authenticator https://github.com/kubernetes-sigs/aws-iam-authenticator/releases/download/v0.6.22/aws-iam-authenticator_0.6.22_linux_amd64
          chmod +x aws-iam-authenticator && sudo mv aws-iam-authenticator /usr/local/bin/

      # -- cloudformation deploy left as-is, but ensure infra/eks-stack.yaml is a real CFN template --

      - name: Fetch ECR URIs (CFN outputs preferred)
        id: ecr
        run: |
          STACK_NAME=${CLUSTER_NAME}-stack
          FRONTEND=""
          ISSUANCE=""
          VERIFICATION=""
          if aws cloudformation describe-stacks --stack-name $STACK_NAME >/dev/null 2>&1; then
            FRONTEND=$(aws cloudformation describe-stacks --stack-name $STACK_NAME \
              --query 'Stacks[0].Outputs[?OutputKey==`FrontendEcrUri`].OutputValue' --output text 2>/dev/null || echo "")
            ISSUANCE=$(aws cloudformation describe-stacks --stack-name $STACK_NAME \
              --query 'Stacks[0].Outputs[?OutputKey==`IssuanceEcrUri`].OutputValue' --output text 2>/dev/null || echo "")
            VERIFICATION=$(aws cloudformation describe-stacks --stack-name $STACK_NAME \
              --query 'Stacks[0].Outputs[?OutputKey==`VerificationEcrUri`].OutputValue' --output text 2>/dev/null || echo "")
          fi
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          [ -z "$FRONTEND" ] && FRONTEND="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${CLUSTER_NAME}-frontend"
          [ -z "$ISSUANCE" ] && ISSUANCE="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${CLUSTER_NAME}-issuance"
          [ -z "$VERIFICATION" ] && VERIFICATION="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${CLUSTER_NAME}-verification"
          echo "frontend=$FRONTEND" >> $GITHUB_OUTPUT
          echo "issuance=$ISSUANCE" >> $GITHUB_OUTPUT
          echo "verification=$VERIFICATION" >> $GITHUB_OUTPUT

      # ECR login, build, push, update kubectl images (use kubectl set image rather than sed)
      - name: Build and push backend images
        run: |
          set -euo pipefail
          ISSUANCE_URI=${{ steps.ecr.outputs.issuance }}
          VERIFICATION_URI=${{ steps.ecr.outputs.verification }}
          if [ "$DEPLOY_ISSUANCE" = "true" ]; then
            docker build -t $ISSUANCE_URI:latest ./backend/issuance-service
            docker push $ISSUANCE_URI:latest
          fi
          if [ "$DEPLOY_VERIFICATION" = "true" ]; then
            docker build -t $VERIFICATION_URI:latest ./backend/verification-service
            docker push $VERIFICATION_URI:latest
          fi

      - name: Deploy backend services to Kubernetes
        run: |
          ISSUANCE_URI=${{ steps.ecr.outputs.issuance }}
          VERIFICATION_URI=${{ steps.ecr.outputs.verification }}
          kubectl apply -f k8s/mongo-deployment.yaml || true
          if [ "$DEPLOY_ISSUANCE" = "true" ]; then
            kubectl apply -f k8s/issuance-deployment.yaml
            kubectl set image deployment/issuance-deployment issuance=${ISSUANCE_URI}:latest --record || true
            kubectl rollout status deployment/issuance-deployment --timeout=300s || true
          fi
          if [ "$DEPLOY_VERIFICATION" = "true" ]; then
            kubectl apply -f k8s/verification-deployment.yaml
            kubectl set image deployment/verification-deployment verification=${VERIFICATION_URI}:latest --record || true
            kubectl rollout status deployment/verification-deployment --timeout=300s || true
          fi
