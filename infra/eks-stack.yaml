# # AWSTemplateFormatVersion: '2010-09-09'
# # Description: Minimal EKS cluster with managed node group and ECR repos for zupple project

# # Parameters:
# #   ClusterName:
# #     Type: String
# #     Default: zupple-eks
# #   KubernetesVersion:
# #     Type: String
# #     Default: '1.29'
# #   NodeInstanceType:
# #     Type: String
# #     Default: t3.medium
# #   NodeDesiredSize:
# #     Type: Number
# #     Default: 2
# #   NodeMinSize:
# #     Type: Number
# #     Default: 1
# #   NodeMaxSize:
# #     Type: Number
# #     Default: 3
# #   VpcCidr:
# #     Type: String
# #     Default: 10.0.0.0/16

# # Resources:
# #   VPC:
# #     Type: AWS::EC2::VPC
# #     Properties:
# #       CidrBlock: !Ref VpcCidr
# #       EnableDnsSupport: true
# #       EnableDnsHostnames: true
# #       Tags:
# #         - Key: Name
# #           Value: !Sub '${ClusterName}-vpc'

# #   InternetGateway:
# #     Type: AWS::EC2::InternetGateway

# #   VPCGatewayAttachment:
# #     Type: AWS::EC2::VPCGatewayAttachment
# #     Properties:
# #       VpcId: !Ref VPC
# #       InternetGatewayId: !Ref InternetGateway

# #   PublicSubnet1:
# #     Type: AWS::EC2::Subnet
# #     Properties:
# #       VpcId: !Ref VPC
# #       CidrBlock: 10.0.0.0/20
# #       AvailabilityZone: !Select [ 0, !GetAZs '' ]
# #       MapPublicIpOnLaunch: true
# #       Tags:
# #         - Key: Name
# #           Value: !Sub '${ClusterName}-public-1'
# #         - Key: kubernetes.io/role/elb
# #           Value: '1'

# #   PublicSubnet2:
# #     Type: AWS::EC2::Subnet
# #     Properties:
# #       VpcId: !Ref VPC
# #       CidrBlock: 10.0.16.0/20
# #       AvailabilityZone: !Select [ 1, !GetAZs '' ]
# #       MapPublicIpOnLaunch: true
# #       Tags:
# #         - Key: Name
# #           Value: !Sub '${ClusterName}-public-2'
# #         - Key: kubernetes.io/role/elb
# #           Value: '1'

# #   PrivateSubnet1:
# #     Type: AWS::EC2::Subnet
# #     Properties:
# #       VpcId: !Ref VPC
# #       CidrBlock: 10.0.32.0/20
# #       AvailabilityZone: !Select [ 0, !GetAZs '' ]
# #       MapPublicIpOnLaunch: false
# #       Tags:
# #         - Key: Name
# #           Value: !Sub '${ClusterName}-private-1'
# #         - Key: kubernetes.io/role/internal-elb
# #           Value: '1'

# #   PrivateSubnet2:
# #     Type: AWS::EC2::Subnet
# #     Properties:
# #       VpcId: !Ref VPC
# #       CidrBlock: 10.0.48.0/20
# #       AvailabilityZone: !Select [ 1, !GetAZs '' ]
# #       MapPublicIpOnLaunch: false
# #       Tags:
# #         - Key: Name
# #           Value: !Sub '${ClusterName}-private-2'
# #         - Key: kubernetes.io/role/internal-elb
# #           Value: '1'

# #   PublicRouteTable:
# #     Type: AWS::EC2::RouteTable
# #     Properties:
# #       VpcId: !Ref VPC

# #   PublicRoute:
# #     Type: AWS::EC2::Route
# #     Properties:
# #       RouteTableId: !Ref PublicRouteTable
# #       DestinationCidrBlock: 0.0.0.0/0
# #       GatewayId: !Ref InternetGateway

# #   PublicSubnet1RouteTableAssociation:
# #     Type: AWS::EC2::SubnetRouteTableAssociation
# #     Properties:
# #       RouteTableId: !Ref PublicRouteTable
# #       SubnetId: !Ref PublicSubnet1

# #   PublicSubnet2RouteTableAssociation:
# #     Type: AWS::EC2::SubnetRouteTableAssociation
# #     Properties:
# #       RouteTableId: !Ref PublicRouteTable
# #       SubnetId: !Ref PublicSubnet2

# #   EKSClusterRole:
# #     Type: AWS::IAM::Role
# #     Properties:
# #       AssumeRolePolicyDocument:
# #         Version: '2012-10-17'
# #         Statement:
# #           - Effect: Allow
# #             Principal:
# #               Service: eks.amazonaws.com
# #             Action: sts:AssumeRole
# #       ManagedPolicyArns:
# #         - arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
# #         - arn:aws:iam::aws:policy/AmazonEKSServicePolicy

# #   EKSCluster:
# #     Type: AWS::EKS::Cluster
# #     Properties:
# #       Name: !Ref ClusterName
# #       Version: !Ref KubernetesVersion
# #       RoleArn: !GetAtt EKSClusterRole.Arn
# #       ResourcesVpcConfig:
# #         SubnetIds:
# #           - !Ref PublicSubnet1
# #           - !Ref PublicSubnet2
# #           - !Ref PrivateSubnet1
# #           - !Ref PrivateSubnet2
# #         EndpointPublicAccess: true
# #         EndpointPrivateAccess: false

# #   NodeInstanceRole:
# #     Type: AWS::IAM::Role
# #     Properties:
# #       AssumeRolePolicyDocument:
# #         Version: '2012-10-17'
# #         Statement:
# #           - Effect: Allow
# #             Principal:
# #               Service: ec2.amazonaws.com
# #             Action: sts:AssumeRole
# #       ManagedPolicyArns:
# #         - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
# #         - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
# #         - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
# #         - arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy

# #   NodeGroup:
# #     Type: AWS::EKS::Nodegroup
# #     Properties:
# #       ClusterName: !Ref ClusterName
# #       NodeRole: !GetAtt NodeInstanceRole.Arn
# #       ScalingConfig:
# #         DesiredSize: !Ref NodeDesiredSize
# #         MinSize: !Ref NodeMinSize
# #         MaxSize: !Ref NodeMaxSize
# #       Subnets:
# #         - !Ref PublicSubnet1
# #         - !Ref PublicSubnet2
# #       InstanceTypes:
# #         - !Ref NodeInstanceType

# #   EbsCsiAddon:
# #     Type: AWS::EKS::Addon
# #     Properties:
# #       ClusterName: !Ref ClusterName
# #       AddonName: aws-ebs-csi-driver
# #       ResolveConflicts: NONE

# #   FrontendEcr:
# #     Type: AWS::ECR::Repository
# #     Properties:
# #       RepositoryName: zupple-frontend
# #       ImageScanningConfiguration:
# #         ScanOnPush: true

# #   IssuanceEcr:
# #     Type: AWS::ECR::Repository
# #     Properties:
# #       RepositoryName: zupple-issuance
# #       ImageScanningConfiguration:
# #         ScanOnPush: true

# #   VerificationEcr:
# #     Type: AWS::ECR::Repository
# #     Properties:
# #       RepositoryName: zupple-verification
# #       ImageScanningConfiguration:
# #         ScanOnPush: true

# # Outputs:
# #   ClusterNameOut:
# #     Value: !Ref ClusterName
# #     Export:
# #       Name: !Sub '${ClusterName}-name'
# #   ClusterArn:
# #     Value: !Ref EKSCluster
# #   ClusterOidcIssuer:
# #     Value: !GetAtt EKSCluster.OpenIdConnectIssuerUrl
# #   PublicSubnetIds:
# #     Value: !Join [ ',', [ !Ref PublicSubnet1, !Ref PublicSubnet2 ] ]
# #   PrivateSubnetIds:
# #     Value: !Join [ ',', [ !Ref PrivateSubnet1, !Ref PrivateSubnet2 ] ]
# #   FrontendEcrUri:
# #     Value: !Sub '${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/zupple-frontend'
# #   IssuanceEcrUri:
# #     Value: !Sub '${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/zupple-issuance'
# #   VerificationEcrUri:
# #     Value: !Sub '${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/zupple-verification'



# # name: Deploy to EKS

# # on:
# #   workflow_dispatch:
# #     inputs:
# #       aws-region:
# #         description: 'AWS region'
# #         default: 'ap-south-1'
# #         required: true
# #       cluster-name:
# #         description: 'EKS Cluster name'
# #         default: 'zupple-eks'
# #         required: true
# #       node-instance-type:
# #         description: 'EC2 instance type for nodes'
# #         default: 't3.medium'
# #         required: true

# # jobs:
# #   deploy:
# #     runs-on: ubuntu-latest
# #     steps:
# #       - name: Checkout
# #         uses: actions/checkout@v4

# #       - name: Configure AWS credentials
# #         uses: aws-actions/configure-aws-credentials@v2
# #         with:
# #           aws-region: ${{ github.event.inputs['aws-region'] }}
# #           role-to-assume: ${{ secrets.AWS_ROLE_ARN }}    # or use access key inputs/secrets
# #           aws-web-identity-token-file: ${{ secrets.AWS_WEB_IDENTITY_TOKEN_FILE }} # example

# #       - name: Create / update CloudFormation stack
# #         run: |
# #           aws cloudformation deploy \
# #             --stack-name "${{ github.event.inputs['cluster-name'] }}-stack" \
# #             --region "${{ github.event.inputs['aws-region'] }}" \
# #             --template-file eks-stack.yaml \
# #             --parameter-overrides ClusterName="${{ github.event.inputs['cluster-name'] }}" \
# #                                   NodeInstanceType="${{ github.event.inputs['node-instance-type'] }}" \
# #                                   NodeDesiredSize=2 \
# #             --capabilities CAPABILITY_NAMED_IAM


# name: Deploy to EKS

# on:
#   workflow_dispatch:
#     inputs:
#       aws_region:
#         description: AWS region
#         required: true
#         default: us-east-1
#       cluster_name:
#         description: EKS cluster name
#         required: true
#         default: zupple-eks
#       node_instance_type:
#         description: Node instance type
#         required: true
#         default: t3.medium
#       deploy_frontend:
#         description: Deploy frontend component
#         required: true
#         default: 'false'
#         type: choice
#         options: ['true','false']
#       deploy_issuance:
#         description: Deploy issuance backend
#         required: true
#         default: 'true'
#         type: choice
#         options: ['true','false']
#       deploy_verification:
#         description: Deploy verification backend
#         required: true
#         default: 'true'
#         type: choice
#         options: ['true','false']
#       issuance_url:
#         description: Issuance backend URL (for frontend build)
#         required: false
#         default: ''
#       verification_url:
#         description: Verification backend URL (for frontend build)
#         required: false
#         default: ''

# jobs:
#   deploy:
#     runs-on: ubuntu-latest
#     permissions:
#       id-token: write
#       contents: read
#     env:
#       AWS_REGION: ${{ github.event.inputs.aws_region }}
#       CLUSTER_NAME: ${{ github.event.inputs.cluster_name }}
#       DEPLOY_FRONTEND: ${{ github.event.inputs.deploy_frontend }}
#       DEPLOY_ISSUANCE: ${{ github.event.inputs.deploy_issuance }}
#       DEPLOY_VERIFICATION: ${{ github.event.inputs.deploy_verification }}

#     steps:
#       - name: Checkout
#         uses: actions/checkout@v4

#       - name: Configure AWS Credentials
#         uses: aws-actions/configure-aws-credentials@v4
#         with:
#           aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
#           aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
#           aws-region: ${{ env.AWS_REGION }}

#       - name: Install deps (kubectl/jq)
#         run: |
#           sudo apt-get update -y
#           sudo apt-get install -y jq
#           curl -sSLo kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.29.0/2024-08-12/bin/linux/amd64/kubectl
#           chmod +x kubectl && sudo mv kubectl /usr/local/bin/
#           curl -sSLo aws-iam-authenticator https://github.com/kubernetes-sigs/aws-iam-authenticator/releases/download/v0.6.22/aws-iam-authenticator_0.6.22_linux_amd64
#           chmod +x aws-iam-authenticator && sudo mv aws-iam-authenticator /usr/local/bin/

#       # -- cloudformation deploy left as-is, but ensure infra/eks-stack.yaml is a real CFN template --

#       - name: Fetch ECR URIs (CFN outputs preferred)
#         id: ecr
#         run: |
#           STACK_NAME=${CLUSTER_NAME}-stack
#           FRONTEND=""
#           ISSUANCE=""
#           VERIFICATION=""
#           if aws cloudformation describe-stacks --stack-name $STACK_NAME >/dev/null 2>&1; then
#             FRONTEND=$(aws cloudformation describe-stacks --stack-name $STACK_NAME \
#               --query 'Stacks[0].Outputs[?OutputKey==`FrontendEcrUri`].OutputValue' --output text 2>/dev/null || echo "")
#             ISSUANCE=$(aws cloudformation describe-stacks --stack-name $STACK_NAME \
#               --query 'Stacks[0].Outputs[?OutputKey==`IssuanceEcrUri`].OutputValue' --output text 2>/dev/null || echo "")
#             VERIFICATION=$(aws cloudformation describe-stacks --stack-name $STACK_NAME \
#               --query 'Stacks[0].Outputs[?OutputKey==`VerificationEcrUri`].OutputValue' --output text 2>/dev/null || echo "")
#           fi
#           AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
#           [ -z "$FRONTEND" ] && FRONTEND="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${CLUSTER_NAME}-frontend"
#           [ -z "$ISSUANCE" ] && ISSUANCE="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${CLUSTER_NAME}-issuance"
#           [ -z "$VERIFICATION" ] && VERIFICATION="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${CLUSTER_NAME}-verification"
#           echo "frontend=$FRONTEND" >> $GITHUB_OUTPUT
#           echo "issuance=$ISSUANCE" >> $GITHUB_OUTPUT
#           echo "verification=$VERIFICATION" >> $GITHUB_OUTPUT

#       - name: Build and push backend images
#         run: |
#           set -euo pipefail
#           ISSUANCE_URI=${{ steps.ecr.outputs.issuance }}
#           VERIFICATION_URI=${{ steps.ecr.outputs.verification }}
#           if [ "$DEPLOY_ISSUANCE" = "true" ]; then
#             docker build -t $ISSUANCE_URI:latest ./backend/issuance-service
#             docker push $ISSUANCE_URI:latest
#           fi
#           if [ "$DEPLOY_VERIFICATION" = "true" ]; then
#             docker build -t $VERIFICATION_URI:latest ./backend/verification-service
#             docker push $VERIFICATION_URI:latest
#           fi

#       - name: Deploy backend services to Kubernetes
#         run: |
#           ISSUANCE_URI=${{ steps.ecr.outputs.issuance }}
#           VERIFICATION_URI=${{ steps.ecr.outputs.verification }}
#           kubectl apply -f k8s/mongo-deployment.yaml || true
#           if [ "$DEPLOY_ISSUANCE" = "true" ]; then
#             kubectl apply -f k8s/issuance-deployment.yaml
#             kubectl set image deployment/issuance-deployment issuance=${ISSUANCE_URI}:latest --record || true
#             kubectl rollout status deployment/issuance-deployment --timeout=300s || true
#           fi
#           if [ "$DEPLOY_VERIFICATION" = "true" ]; then
#             kubectl apply -f k8s/verification-deployment.yaml
#             kubectl set image deployment/verification-deployment verification=${VERIFICATION_URI}:latest --record || true
#             kubectl rollout status deployment/verification-deployment --timeout=300s || true
#           fi


name: Deploy to EKS

on:
  workflow_dispatch:
    inputs:
      aws_region:
        description: AWS region
        required: true
        default: ap-south-1
      cluster_name:
        description: EKS cluster name
        required: true
        default: zupple-eks
      node_instance_type:
        description: Node instance type
        required: true
        default: t3.medium
      deploy_frontend:
        description: Deploy frontend component
        required: true
        default: 'false'
        type: choice
        options: ['true','false']
      deploy_issuance:
        description: Deploy issuance backend
        required: true
        default: 'true'
        type: choice
        options: ['true','false']
      deploy_verification:
        description: Deploy verification backend
        required: true
        default: 'true'
        type: choice
        options: ['true','false']
      issuance_url:
        description: Issuance backend URL (optional) - used to build frontend if provided
        required: false
        default: ''
      verification_url:
        description: Verification backend URL (optional) - used to build frontend if provided
        required: false
        default: ''

jobs:
  deploy:
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    env:
      AWS_REGION: ${{ github.event.inputs.aws_region }}
      CLUSTER_NAME: ${{ github.event.inputs.cluster_name }}
      DEPLOY_FRONTEND: ${{ github.event.inputs.deploy_frontend }}
      DEPLOY_ISSUANCE: ${{ github.event.inputs.deploy_issuance }}
      DEPLOY_VERIFICATION: ${{ github.event.inputs.deploy_verification }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set IMAGE_TAG
        run: echo "IMAGE_TAG=$(echo ${GITHUB_SHA} | cut -c1-7)" >> $GITHUB_ENV

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install utilities (kubectl, eksctl, jq)
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq curl
          # kubectl (pinned to a version compatible with EKS 1.29)
          curl -sSLo kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.29.0/2024-08-12/bin/linux/amd64/kubectl
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/
          # eksctl
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin/
          eksctl version
          kubectl version --client --short || true

      - name: Resolve ECR URIs (account/region-based)
        id: ecr
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          REG=${{ env.AWS_REGION }}
          CLUSTER=${{ env.CLUSTER_NAME }}
          echo "AWS_ACCOUNT=${AWS_ACCOUNT_ID}"
          FRONTEND_URI="${AWS_ACCOUNT_ID}.dkr.ecr.${REG}.amazonaws.com/${CLUSTER}-frontend"
          ISSUANCE_URI="${AWS_ACCOUNT_ID}.dkr.ecr.${REG}.amazonaws.com/${CLUSTER}-issuance"
          VERIFICATION_URI="${AWS_ACCOUNT_ID}.dkr.ecr.${REG}.amazonaws.com/${CLUSTER}-verification"
          echo "frontend=$FRONTEND_URI" >> $GITHUB_OUTPUT
          echo "issuance=$ISSUANCE_URI" >> $GITHUB_OUTPUT
          echo "verification=$VERIFICATION_URI" >> $GITHUB_OUTPUT

      - name: ECR Login
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          REGISTRY="${AWS_ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com"
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin $REGISTRY

      - name: Ensure ECR repositories exist
        run: |
          set -euo pipefail
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          REGION=${{ env.AWS_REGION }}
          CLUSTER=${{ env.CLUSTER_NAME }}
          declare -a REPOS=()
          if [ "${DEPLOY_FRONTEND}" = "true" ]; then REPOS+=("${CLUSTER}-frontend"); fi
          if [ "${DEPLOY_ISSUANCE}" = "true" ]; then REPOS+=("${CLUSTER}-issuance"); fi
          if [ "${DEPLOY_VERIFICATION}" = "true" ]; then REPOS+=("${CLUSTER}-verification"); fi
          for NAME in "${REPOS[@]}"; do
            echo "Processing ECR repo: $NAME"
            if aws ecr describe-repositories --repository-names "$NAME" --region "$REGION" >/dev/null 2>&1; then
              echo "OK: $NAME exists"
            else
              echo "Creating ECR repo: $NAME"
              aws ecr create-repository --repository-name "$NAME" --region "$REGION" || true
              for i in {1..10}; do
                if aws ecr describe-repositories --repository-names "$NAME" --region "$REGION" >/dev/null 2>&1; then
                  echo "Confirmed: $NAME available"
                  break
                fi
                sleep 2
              done
            fi
          done

      - name: Create EKS cluster if missing (eksctl)
        id: create_eks
        run: |
          set -euo pipefail
          CLUSTER=${{ env.CLUSTER_NAME }}
          REGION=${{ env.AWS_REGION }}
          NODE_TYPE=${{ github.event.inputs.node_instance_type }}
          if aws eks describe-cluster --name "$CLUSTER" --region "$REGION" >/dev/null 2>&1; then
            echo "Cluster $CLUSTER exists"
          else
            echo "Cluster $CLUSTER not found. Creating with eksctl (this may take several minutes)..."
            eksctl create cluster --name "$CLUSTER" --region "$REGION" --node-type "$NODE_TYPE" --nodes 2 --nodes-min 1 --nodes-max 3
          fi

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

      - name: Debug: show resolved URIs & AWS account
        run: |
          echo "Account: $(aws sts get-caller-identity --query Account --output text)"
          echo "Region: ${{ env.AWS_REGION }}"
          echo "Cluster: ${{ env.CLUSTER_NAME }}"
          echo "Image tag: $IMAGE_TAG"
          echo "ECR frontend: ${{ steps.ecr.outputs.frontend }}"
          echo "ECR issuance: ${{ steps.ecr.outputs.issuance }}"
          echo "ECR verification: ${{ steps.ecr.outputs.verification }}"

      - name: Build & push backend images
        run: |
          set -euo pipefail
          ISSUANCE_URI=${{ steps.ecr.outputs.issuance }}
          VERIFICATION_URI=${{ steps.ecr.outputs.verification }}
          TAG=$IMAGE_TAG

          if [ "${DEPLOY_ISSUANCE}" = "true" ]; then
            echo "Building issuance -> ${ISSUANCE_URI}:${TAG}"
            docker build -t ${ISSUANCE_URI}:${TAG} ./backend/issuance-service
            docker push ${ISSUANCE_URI}:${TAG}
          else
            echo "Skipping issuance build"
          fi

          if [ "${DEPLOY_VERIFICATION}" = "true" ]; then
            echo "Building verification -> ${VERIFICATION_URI}:${TAG}"
            docker build -t ${VERIFICATION_URI}:${TAG} ./backend/verification-service
            docker push ${VERIFICATION_URI}:${TAG}
          else
            echo "Skipping verification build"
          fi

      - name: Apply base k8s manifests (services / pvc / mongo if present)
        run: |
          # apply base manifests that don't depend on image tag
          kubectl apply -f k8s/namespace.yaml || true
          kubectl apply -f k8s/issuance-service.yaml || true
          kubectl apply -f k8s/verification-service.yaml || true
          # apply PVCs if present
          kubectl apply -f k8s/issuance-pvc.yaml || true
          kubectl apply -f k8s/verification-pvc.yaml || true

      - name: Deploy backend deployments + set images
        run: |
          set -euo pipefail
          ISSUANCE_URI=${{ steps.ecr.outputs.issuance }}
          VERIFICATION_URI=${{ steps.ecr.outputs.verification }}
          TAG=$IMAGE_TAG

          if [ "${DEPLOY_ISSUANCE}" = "true" ]; then
            # apply deployment (keeps a placeholder image)
            kubectl apply -f k8s/issuance-deployment.yaml
            # update image reliably
            kubectl set image deployment/issuance-deployment issuance=${ISSUANCE_URI}:${TAG} --record
            kubectl rollout status deployment/issuance-deployment --timeout=300s
          fi

          if [ "${DEPLOY_VERIFICATION}" = "true" ]; then
            kubectl apply -f k8s/verification-deployment.yaml
            kubectl set image deployment/verification-deployment verification=${VERIFICATION_URI}:${TAG} --record
            kubectl rollout status deployment/verification-deployment --timeout=300s
          fi

      - name: Get backend service URLs
        id: backend_urls
        run: |
          ISSUANCE_URL="${{ github.event.inputs.issuance_url }}"
          VERIFICATION_URL="${{ github.event.inputs.verification_url }}"

          # If not provided, attempt to read LoadBalancer hostnames/IPs
          if [ -z "$ISSUANCE_URL" ] && [ "${DEPLOY_ISSUANCE}" = "true" ]; then
            for i in {1..30}; do
              IP=$(kubectl get svc issuance-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
              [ -z "$IP" ] && IP=$(kubectl get svc issuance-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || true)
              if [ -n "$IP" ]; then
                ISSUANCE_URL="http://${IP}:4001"
                break
              fi
              sleep 5
            done
          fi

          if [ -z "$VERIFICATION_URL" ] && [ "${DEPLOY_VERIFICATION}" = "true" ]; then
            for i in {1..30}; do
              IP=$(kubectl get svc verification-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
              [ -z "$IP" ] && IP=$(kubectl get svc verification-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || true)
              if [ -n "$IP" ]; then
                VERIFICATION_URL="http://${IP}:4002"
                break
              fi
              sleep 5
            done
          fi

          # fallbacks (internal cluster service names)
          [ -z "$ISSUANCE_URL" ] && ISSUANCE_URL="http://issuance-service:4001"
          [ -z "$VERIFICATION_URL" ] && VERIFICATION_URL="http://verification-service:4002"

          echo "issuance_url=$ISSUANCE_URL" >> $GITHUB_OUTPUT
          echo "verification_url=$VERIFICATION_URL" >> $GITHUB_OUTPUT
          echo "Issuance URL: $ISSUANCE_URL"
          echo "Verification URL: $VERIFICATION_URL"

      - name: Build & push frontend (optional)
        if: env.DEPLOY_FRONTEND == 'true'
        run: |
          set -euo pipefail
          FRONTEND_URI=${{ steps.ecr.outputs.frontend }}
          TAG=$IMAGE_TAG
          ISSUANCE_URL=${{ steps.backend_urls.outputs.issuance_url }}
          VERIFICATION_URL=${{ steps.backend_urls.outputs.verification_url }}
          echo "Building frontend with VITE_ISSUANCE_URL=$ISSUANCE_URL and VITE_VERIFY_URL=$VERIFICATION_URL"
          docker build --build-arg VITE_ISSUANCE_URL="$ISSUANCE_URL" --build-arg VITE_VERIFY_URL="$VERIFICATION_URL" -t ${FRONTEND_URI}:${TAG} ./frontend
          docker push ${FRONTEND_URI}:${TAG}

      - name: Deploy frontend (optional)
        if: env.DEPLOY_FRONTEND == 'true'
        run: |
          FRONTEND_URI=${{ steps.ecr.outputs.frontend }}
          TAG=$IMAGE_TAG
          kubectl apply -f k8s/frontend-deployment.yaml
          kubectl set image deployment/frontend-deployment frontend=${FRONTEND_URI}:${TAG} --record
          kubectl rollout status deployment/frontend-deployment --timeout=300s

      - name: Show services
        run: |
          echo "=== Services ==="
          kubectl get svc -o wide
          echo "=== Pods ==="
          kubectl get pods -o wide
